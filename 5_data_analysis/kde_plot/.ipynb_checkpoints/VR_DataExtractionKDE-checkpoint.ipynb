{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "from scipy import stats\n",
                "import multiprocessing\n",
                "import os\n",
                "import pandas as pd\n",
                "from parallel_function import KDE\n",
                "import time\n",
                "import pickle\n",
                "import json\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_files_and_info(worker,ipath):\n",
                "    n_expected=7 # header + folder [0-5]\n",
                "    results={}\n",
                "    npath=ipath+worker\n",
                "    sessions=os.listdir(npath)\n",
                "    sessionID=-1\n",
                "    m=\"\"\n",
                "    if len(sessions) == 1:\n",
                "        npath=ipath+\"{}/{}\".format(worker,sessions[0])\n",
                "        if len(os.listdir(npath)) ==7:\n",
                "            sessionID=sessions[0]\n",
                "            m+=\"OK \"\n",
                "        else:\n",
                "            m+=\"SingleSession Incomplete \"\n",
                "    else:\n",
                "        success=0\n",
                "        m+=\"Multiple Sessions \"\n",
                "        for s in sessions:\n",
                "            npath=ipath+\"{}/{}\".format(worker,s)\n",
                "            if len(os.listdir(npath)) ==7:\n",
                "                success+=1\n",
                "                sessionID=s\n",
                "        if success == 1:\n",
                "            m+=\"one OK \"\n",
                "        else:\n",
                "            sessionID=-1\n",
                "            m+=str(success)+\" Times completed \"\n",
                "            print(npath, m)\n",
                "    if sessionID!=-1:\n",
                "        npath=ipath+\"{}/{}/\".format(worker,sessionID)\n",
                "        fname=\"{}_header.json\".format(worker,sessionID,worker)\n",
                "        with open(npath+fname, 'r') as f:\n",
                "            parsed_json = json.load(f)\n",
                "        fileList=[]\n",
                "        \n",
                "        for tnum in os.listdir(npath):\n",
                "            trial={}\n",
                "            test_path = os.path.join(npath, tnum)\n",
                "            if os.path.isdir(test_path):\n",
                "                filename=\"{}_trial_{}_CORRECT.csv\".format(worker,tnum)\n",
                "                if os.path.isfile(os.path.join(test_path, filename)):\n",
                "                    header=\"{}_trial_{}_PH.json\".format(worker,tnum)\n",
                "                    tail=\"{}_trial_{}_PT.json\".format(worker,tnum)\n",
                "                    with open(os.path.join(test_path, header), 'r') as f:\n",
                "                        parsed_json1 = json.load(f)\n",
                "                    with open(os.path.join(test_path, tail), 'r') as f:\n",
                "                        parsed_json2 = json.load(f)\n",
                "                    meta = {**parsed_json1 , **parsed_json2}                \n",
                "                    fileList.append({\"trial\":int(tnum),\"path\": test_path,\"filename\":filename,\"meta\":meta})\n",
                "        results['info']=parsed_json\n",
                "        results['files']=fileList\n",
                "    results['state']=m\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "def ProcessDataframe(fname):\n",
                "    df=pd.read_csv(fname,names=[\"Time\", \"x\", \"y\", \"z\",\"Azimuth\",\"Elevation\"])\n",
                "    df = df.apply(pd.to_numeric, errors='coerce')\n",
                "    df = df.dropna()\n",
                "    df = df.drop('Time', 1)\n",
                "    df = df.drop('Azimuth', 1)\n",
                "    df = df.drop('Elevation', 1)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "allpaths=[\"./raw_dataExp1/results/\",\"./raw_dataExp2/results/\",\"./raw_dataExp3/results/\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": [
                "all_merged=[pd.DataFrame() for i in range(3)]\n",
                "for ipath in allpaths:\n",
                "    print(ipath)\n",
                "    ignored=0\n",
                "    \n",
                "    workerIDs=os.listdir(ipath)\n",
                "    entry_num=0\n",
                "    for worker in workerIDs:\n",
                "        entry=get_files_and_info(worker,ipath)\n",
                "        if 'info' in entry:\n",
                "            groupIDX=int(entry['info']['gr'])-1\n",
                "            for file in entry['files']:\n",
                "                filename=file['filename']\n",
                "                fname=os.path.join(file['path'], filename)\n",
                "                df=ProcessDataframe(fname)\n",
                "                df['task']=int(file['meta']['td'])\n",
                "                df['entry_num']=entry_num\n",
                "                entry_num+=1\n",
                "                all_merged[groupIDX]=all_merged[groupIDX].append(df,ignore_index=True)\n",
                "        else:\n",
                "            ignored+=1\n",
                "all_merged[0].to_csv(\"VR_merged_data_BASE.csv\",index=False)\n",
                "all_merged[1].to_csv(\"VR_merged_data_ATRIA.csv\",index=False)\n",
                "all_merged[2].to_csv(\"VR_merged_data_GLASS.csv\",index=False)\n",
                "print(\"Total Ignored\",ignored)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 0,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "data_analysis",
            "language": "python",
            "name": "data_analysis"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}